## (ML 9.1) Linear regression - nonlinearity via basis functions (기저함수를 통한 비선형성)
1. 단순히 직선/평면의 방정식을 구하는 것보다 일반적인 개념이다
    * 2개의 숫자 데이터 집합을 통해 상관관계 혹은 추세를 예측하기 위한 직선의 기울기를 구하는게 전부는 아니라 결과가 곡선이나 평면일 수도 있다는 의미
    * [위키:선형회귀] (https://ko.wikipedia.org/wiki/선형_회귀)
1. 세상에는 정말 많은 기저함수가 존재하며, 그냥 가져다 쓰면 된다
    * Identity, General basis function Pi, Polinomial, Radial, Fourier, Wavelets 등 
1. 기저함수 자체는 선형성을 가지지 않아도 되며, 새로운 데이터 *x*에 대한 *y*를 예측하는 함수를 선택하는 것이 목적이다
    * 목적은 새로운 입력에 대한 출력 y를 예측하는 것이므로, 입력 데이터 x에 어떠한 변환(차원축소)이 발생해도 괜찮다
        * 그러면 x의 차원은 바뀌어도 되지만 y의 차원은 바뀌면 안 되는 것 아닌가?
    * 기저함수 Pi는 독립변수 x에 대한 함수이지, 종속변수 y에 대한 함수가 아니며, 가중치 w 또한 x에 따라 변하는 계수이다.
        * 종속변수 y는 차원이 항상 1차원이어야 하고, n차원의 결과는 선형회귀로는 구하기 힘든가?
    * 반대로 입력이 1개일 때에, 출력이 여러개라는 것은 함수가 아니라는 말이 되므로 어려울 것 같으며, 사람이 직관적으로 이해하기 어려울 것 같다.
    * 또한 n : m 보다 n : 1 * m 으로 따로 구하는 것이 더 좋은 결과가 나올 것 같은데... 잘 모르겠다
        * 기저함수의 역함수를 구할수만 있다면 차원을 되돌릴 수 있으므로 가능할 것도 같다.
    * 일반적인 경우에 차원이 축소되면 정보가 사라지기 때문에, 역함수를 구하기 어려울 것 같고, 무손실압축도 있으므로 될 것도 같고 모르겠다.


### 선형성에 대한 정의
1. 임의의 수 x, y에 대해 아래의 2개의 식이 항상 성립하는 경우 함수 f는 *선형*이라고 한다
* f(x+y) = f(x) + f(y) 
* f(a\*x) = a\*f(x)
> [위키:선형성] (https://ko.wikipedia.org/wiki/%EC%84%A0%ED%98%95%EC%84%B1)


## (ML 9.2) Linear regression - Definition & Motivation (정의 & 동기)
1. 기저함수를 잘 선택하면 Feature Space 상의 데이터 차원이 낮아질 수 있다
> 일반적으로 ML 에서는 학습을 위한 중요한 데이터의 종류를 특질 혹은 피쳐라 표현하는데, 같은 개념인지는 잘 모르겠다
1. identify 함수를 통해 손쉽게 w를 구할 수도 있다

(ML 9.3) Choosing f under linear regression

(ML 9.4 ~ 9.6) MLE for linear regression

(ML 9.7) Basis functions MLE

